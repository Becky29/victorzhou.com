---
title: "Easy Visual Question Answering"
date: "2019-11-30T12:00:00.000Z"
template: "post"
usesKatex: true
draft: false
slug: "/blog/easy-vqa/"
img:
isML: true
category: "Machine Learning"
tags:
  - "Machine Learning"
  - "Neural Networks"
  - "Computer Vision"
  - "Python"
  - "For Beginners"
description: A gentle introduction to Visual Question Answering (VQA) using neural networks.
prev: "/series/neural-networks-from-scratch/"
next: "/blog/keras-cnn-tutorial/"
---

What sport is depicted in this image?

![Image from the CloudCV VQA Demo](./media-link/vqa-post/baseball.jpg)

You probably immediately knew the answer: **baseball**. Easy, right?

Now imagine you're a computer. You're given that same image and the text "_what sport is depicted in this image?_" and asked to produce the answer. Not so easy anymore, is it?

This problem is known as **Visual Question Answering** (VQA): answering open-ended questions about images. This might seem like a pretty unapproachable problem at first, but in reality it's probably more accessible than you think. In this post, we'll **explore basic methods for performing VQA and build our own simple implementation** in Python.

**This post assumes a basic knowledge of Convolutional Neural Networks (CNNs)**. My [introduction to CNNs](/blog/intro-to-cnns-part-1/) covers everything you need to know, so start there if necessary.

## 1. The Dataset

The best known dataset for VQA can be found at [visualqa.org](https://visualqa.org) and contains 200k+ images and over a million questions (with answers) about those images. Here a few examples from the original [VQA paper](https://arxiv.org/pdf/1505.00468.pdf):

![](./media-link/vqa-post/vqa-example.png)

Impressive, right? Unfortunately, this level of VQA is outside of the scope of this blog post. We'll instead be using a custom dataset we created just for this blog post: [easy-VQA](https://github.com/vzhou842/easy-VQA). TODO: link to download, more instructions on setup

The images in the easy-VQA dataset are much simpler:

![9 example images from easy-VQA.](./media-link/vqa-post/easy-vqa-images.png)

The questions are also much simpler:

- What shape is in the image?
- What color is the triangle?
- Is there a green shape in the image?
- Does the image contain a circle?

In total, easy-VQA contains TODO # images and TODO # questions, split into training (80%) and testing (20%) sets. The questions have 13 possible answers:

- **Yes/No**: Yes, No
- **Shapes**: Circle, Rectangle, Triangle
- **Colors**: Red, Green, Blue, Black, Gray, Teal, Brown, Yellow

## 2. The Approach

The standard approach to performing VQA looks something like this:

1. Process the image.
2. Process the question.
3. Combine features from steps 1/2.
4. Assign probabilities to each possible answer.

![An animated visualization of a typical VQA architecture.](/media/vqa-post/architecture.gif)

Notice that we're working with a **fixed answer set** where exactly one of the possible answers is guaranteed to be correct. This makes our lives a lot easier because we don't have to _generate_ the correct answer, we just have to answer what is effectively a **multiple-choice question**. Most cutting-edge VQA systems out there have 1000 possible answers, but for this post we'll only allow the 13 possible answers included in [easy-VQA](https://github.com/vzhou842/easy-VQA).

Steps 1 and 2 generally use methods from [Computer Vision](/tag/computer-vision/) and [Natural Language Processing](/tag/natural-language-processing/), respectively, to turn raw image / text inputs into processed data vectors. These two output representations can then be used analyzed together to ultimately pick the most likely answer.

### An Example

Here's a very simple example of how a VQA system might answer the question _"what color is the triangle?"_ about the image in the visualization above:

1. Look for **shapes** and **colors** in the image. A simple [CNN](/blog/intro-to-cnns-part-1/) could be taught to recognize that our image contains a **triangle** that is **blue**.
2. Understand the **question type**. Since the question begins with _"what color"_, it's easy to realize that the answer should be a color.
3. For each possible answer choice, determine its "strength" based on info from the previous two steps. The answer "Blue" will have a high strength because:
    * we know the image has a blue shape
    * we know the answer should be a color
4. Convert each answer's "strength" to a probability using something like [Softmax](/blog/softmax). The answer "Blue" will have close to 100% probability.

In the following sections, we'll walk through the specifics of implementing each of these 4 steps for our easy-VQA dataset.

## 3. The Image Model



## 4. The Question Model
