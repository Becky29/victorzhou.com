---
title: "Easy Visual Question Answering"
date: "2019-11-30T12:00:00.000Z"
template: "post"
usesKatex: true
draft: false
slug: "/blog/easy-vqa/"
img:
isML: true
category: "Machine Learning"
tags:
  - "Machine Learning"
  - "Neural Networks"
  - "Computer Vision"
  - "Python"
  - "For Beginners"
description: A gentle introduction to Visual Question Answering (VQA) using neural networks.
prev: "/series/neural-networks-from-scratch/"
next: "/blog/keras-cnn-tutorial/"
---

What sport is depicted in this image?

![Image from the CloudCV VQA Demo](./media-link/vqa-post/baseball.jpg)

You probably immediately knew the answer: **baseball**. Easy, right?

Now imagine you're a computer. You're given that same image and the text "_what sport is depicted in this image?_" and asked to produce the answer. Not so easy anymore, is it?

This problem is known as **Visual Question Answering** (VQA): answering open-ended questions about images. This might seem like a pretty unapproachable problem at first, but in reality it's probably more accessible than you think. In this post, we'll **explore basic methods for performing VQA and build our own simple implementation** in Python.

**This post assumes a basic knowledge of Convolutional Neural Networks (CNNs)**. My [introduction to CNNs](/blog/intro-to-cnns-part-1/) covers everything you need to know, so start there if necessary.

## 1. The Dataset

The best known dataset for VQA can be found at [visualqa.org](https://visualqa.org) and contains 200k+ images and over a million questions (with answers) about those images. Here a few examples from the original [VQA paper](https://arxiv.org/pdf/1505.00468.pdf):

![](./media-link/vqa-post/vqa-example.png)

Impressive, right? Unfortunately, this level of VQA is outside of the scope of this blog post. We'll instead be using a custom dataset we created just for this blog post: [easy-VQA](https://github.com/vzhou842/easy-VQA). TODO: link to download, more instructions on setup

The images in the easy-VQA dataset are much simpler:

![9 example images from easy-VQA.](./media-link/vqa-post/easy-vqa-images.png)

The questions are also much simpler:

- What shape is in the image?
- What color is the triangle?
- Is there a green shape in the image?
- Does the image contain a circle?

In total, easy-VQA contains TODO # images and TODO # questions, split into training (80%) and testing (20%) sets. The questions have 13 possible answers:

- **Yes/No**: Yes, No
- **Shapes**: Circle, Rectangle, Triangle
- **Colors**: Red, Green, Blue, Black, Gray, Teal, Brown, Yellow

## 2. The Approach

The standard approach to performing VQA looks something like this:

1. Process the image.
2. Process the question.
3. Combine features from steps 1/2.
4. Assign probabilities to each possible answer.

![An animated visualization of a typical VQA architecture.](/media/vqa-post/architecture.gif)

Notice that we're working with a **fixed answer set** where exactly one of the possible answers is guaranteed to be correct. This makes our lives a lot easier because we don't have to _generate_ the correct answer, we just have to answer what is effectively a **multiple-choice question**. Most cutting-edge VQA systems out there have 1000 possible answers, but for this post we'll only allow the 13 possible answers included in [easy-VQA](https://github.com/vzhou842/easy-VQA).

Steps 1 and 2 generally use methods from [Computer Vision](/tag/computer-vision/) and [Natural Language Processing](/tag/natural-language-processing/), respectively, to turn raw image / text inputs into processed data vectors. These two output representations can then be used analyzed together to ultimately pick the most likely answer.

### An Example

Here's a very simple example of how a VQA system might answer the question _"what color is the triangle?"_ about the image in the visualization above:

1. Look for **shapes** and **colors** in the image. A simple [CNN](/blog/intro-to-cnns-part-1/) could be taught to recognize that our image contains a **triangle** that is **blue**.
2. Understand the **question type**. Since the question begins with _"what color"_, it's easy to realize that the answer should be a color.
3. For each possible answer choice, determine its "strength" based on info from the previous two steps. The answer "Blue" will have a high strength because:
    * we know the image has a blue shape
    * we know the answer should be a color
4. Convert each answer's "strength" to a probability using something like [Softmax](/blog/softmax). The answer "Blue" will have close to 100% probability.

In the following sections, we'll walk through the specifics of implementing each of these 4 steps for our easy-VQA dataset.

## 3. The Image Model

First up: our image model. As we've previously mentioned, we'll build a [Convolutional Neural Network](/blog/intro-to-cnns-part-1/) (CNN) to extract information from the image input. To do this, we'll use [Keras](https://keras.io/), a beginner-friendly but powerful deep learning library for Python. I've already written [a guide on using Keras to implement CNNs](/blog/keras-cnn-tutorial/) - it might help to open it in a new tab or skim it before continuing.

Our image dataset is not very complex, so we don't need a very complicated CNN to tackle it:

![image]()

```python
# code
```

## 4. The Question Model

Next up: our question model. Most VQA models would use some kind of [Recurrent Neural Network](/blog/intro-to-rnns/) (RNN) to process the question input, but that's a little overkill for our use case. The questions in the easy-VQA dataset are short, simple, and come from a fixed set of question templates, so they're much more approachable compared those you might see in the real world.

Instead of a complicated RNN architecture, we'll take a simpler approach:

1. Use a [Bag of Words](/blog/bag-of-words/) (BOW) representation to turn each question into a **vector**.
2. Use that vector as input to a [standard (feedforward) neural network](/blog/intro-to-neural-networks/).

Don't worry if you don't entirely understand what that meant. We'll go through both of those steps below.

### Bag of Words (BOW)

A BOW representation turns any text string into a fixed-length vector by counting how many times each word appears in the text. I've written a [short, beginner-friendly introduction to Bag-of-Words models](/blog/bag-of-words/) - I'd recommend reading that now if you're unfamiliar with them!

If you continue reading, I'm going to assume you have a basic understanding of BOW models.

### Neural Network Time!

As discussed before, our question dataset is relatively simple, so we don't need anything too fancy for our Question Model. We'll just pass our BOW vector representation into 2 **fully-connected** (FC) neural network layers:

![](/media/vqa-post/feedforward.svg)

> Reminder: fully-connected layers have every node connected to every output from the previous layer. We used fully-connected layers in my [intro to Neural Networks](/blog/intro-to-neural-networks/) if you need a refresher.

## 5. The Merge

We'll use a very simple method to merge our image and question vectors: **element-wise multiplication**. Implementing this is a one-liner with Keras's [Multiply merge layer](https://keras.io/layers/merge/):

```python
from keras.layers import Multiply

out = Multiply()([im_out, question_out])
```

The `out` vector now contains information derived from _both_ the image and the question.

### An Example

To illustrate how this might be useful, consider this (somewhat contrived) example:

- The first element in the image vector is **high** when the image contains a blue shape and **low** otherwise.
- The first element in the question vector is **high** when the question contains the word "blue" and **low** otherwise.

Then the first element in the `out` vector will only be high when _both_ the image and the question are related to the color blue. This result would be very useful in answering a question like _"Is there a blue shape in the image?"_

Other merge methods listed under Keras's [Merge Layers](https://keras.io/layers/merge/) section include `Add`, `Subtract`, `Concatenate`, and `Average`, all of which do what you think they do. Most of these merge methods would probably work just as well as `Multiply` for our simple dataset - feel free to try them out on your own!

## 6. The Output

Finally, it's time for our VQA system to produce an answer. Recall that we're working with a **fixed answer set**: we know all possible answers and exactly one is guaranteed to be correct.

For this step, we'll use [Softmax](/blog/softmax) to turn our output values into _probabilities_ so we can quantify how sure we are about each possible answer. If you're unfamiliar with Softmax, I highly recommend reading my [explanation of Softmax](/blog/softmax) before continuing.

Keras comes with Softmax already implemented:

```python
out = Dense(num_answers, activation='softmax')(out)
```

## 7. The Results

## 8. The End


