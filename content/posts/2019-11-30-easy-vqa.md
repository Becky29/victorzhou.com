---
title: "Easy Visual Question Answering"
date: "2019-11-30T12:00:00.000Z"
template: "post"
usesKatex: true
draft: false
slug: "/blog/easy-vqa/"
img:
isML: true
category: "Machine Learning"
tags:
  - "Machine Learning"
  - "Neural Networks"
  - "Computer Vision"
  - "Python"
  - "For Beginners"
description: A gentle introduction to Visual Question Answering (VQA) using neural networks.
prev: "/series/neural-networks-from-scratch/"
next: "/blog/keras-cnn-tutorial/"
---

What sport is depicted in this image?

![Image from the CloudCV VQA Demo](./media-link/vqa-post/baseball.jpg)

You probably immediately knew the answer: **baseball**. Easy, right?

Now imagine you're a computer. You're given that same image and the text "_what sport is depicted in this image?_" and asked to produce the answer. Not so easy anymore, is it?

This problem is known as **Visual Question Answering** (VQA): answering open-ended questions about images. This might seem like a pretty unapproachable problem at first, but in reality it's probably more accessible than you think. In this post, we'll **explore basic methods for performing VQA and build our own simple implementation** in Python.

**This post assumes a basic knowledge of Convolutional Neural Networks (CNNs)**. My [introduction to CNNs](/blog/intro-to-cnns-part-1/) covers everything you need to know, so start there if necessary.

## 1. The Dataset

The best known dataset for VQA can be found at [visualqa.org](https://visualqa.org) and contains 200k+ images and over a million questions (with answers) about those images. Here a few examples from the original [VQA paper](https://arxiv.org/pdf/1505.00468.pdf):

![](./media-link/vqa-post/vqa-example.png)


